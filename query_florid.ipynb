{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9948473,
          "sourceType": "datasetVersion",
          "datasetId": 6117682
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook33c211fecb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sca-opdir/sentinel-data/blob/main/query_florid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "4vdsb0zLzyXs"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "mzufferey_sentinel_path = kagglehub.dataset_download('mzufferey/sentinel')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "36EDuRRszyXu"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from queue import Queue\n",
        "import threading\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "num_threads = os.cpu_count()\n",
        "\n",
        "# Chemin de sortie pour le fichier CSV\n",
        "output_file = \"output_results_1000.csv\"\n",
        "\n",
        "kagout_file = '/kaggle/working/' + output_file\n",
        "if os.path.exists(kagout_file):\n",
        "    os.remove(kagout_file)\n",
        "\n",
        "\n",
        "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"\\n\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-19T09:24:12.938148Z",
          "iopub.execute_input": "2024-11-19T09:24:12.938543Z",
          "iopub.status.idle": "2024-11-19T09:24:13.411484Z",
          "shell.execute_reply.started": "2024-11-19T09:24:12.938507Z",
          "shell.execute_reply": "2024-11-19T09:24:13.410381Z"
        },
        "id": "I2cLfZyWzyXu",
        "outputId": "05f138ab-f435-4e10-e493-c22ac68d35b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "2024-11-19 09:24:13 \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#central_points_url =\"https://github.com/sca-opdir/sentinel-data/raw/refs/heads/main/central_points_avecgeom.xlsx\"\n",
        "central_points_path = \"/kaggle/input/sentinel/central_points_avecgeom.xlsx\"\n",
        "liste_especes_path = \"/kaggle/input/sentinel/liste_espces.xlsx\"\n",
        "#liste_especes_url = \"https://github.com/sca-opdir/sentinel-data/raw/refs/heads/main/liste_esp%C3%A8ces.xlsx\"\n",
        "# df_central_points = pd.read_excel(central_points_url, engine=\"openpyxl\")\n",
        "# df_liste_especes = pd.read_excel(liste_especes_url)\n",
        "df_central_points = pd.read_excel(central_points_path)\n",
        "df_liste_especes = pd.read_excel(liste_especes_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-19T09:24:13.413685Z",
          "iopub.execute_input": "2024-11-19T09:24:13.414171Z",
          "iopub.status.idle": "2024-11-19T09:24:19.059137Z",
          "shell.execute_reply.started": "2024-11-19T09:24:13.414135Z",
          "shell.execute_reply": "2024-11-19T09:24:19.057997Z"
        },
        "id": "MhtEB3WbzyXv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour appeler l'API\n",
        "def get_florID_resp(latcoord, loncoord, ntaxon, reqtaxons,\n",
        "                    reqdate=\"2024-06-20\", apiurl=\"https://speciesid.wsl.ch/florid\"):\n",
        "    url = apiurl\n",
        "    headers = {\n",
        "        \"accept\": \"*/*\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"date\": reqdate,\n",
        "        \"lat\": latcoord,\n",
        "        \"lon\": loncoord,\n",
        "        \"num_taxon_ids\": ntaxon,\n",
        "        \"req_taxon_ids\": reqtaxons\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    if response.status_code == 200:\n",
        "        response_dict = response.json()\n",
        "        return {\n",
        "            \"center_x\": loncoord,\n",
        "            \"center_y\": latcoord,\n",
        "            \"req_taxa\": response_dict['requested_taxa']\n",
        "        }\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Fonction pour traiter chaque point et envoyer les résultats à la file d’attente\n",
        "def process_point(ipoint, output_queue):\n",
        "    ilat = df_central_points.center_y[ipoint]\n",
        "    ilon = df_central_points.center_x[ipoint]\n",
        "    ipoly = df_central_points.polygon_coords[ipoint]\n",
        "\n",
        "    api_resp = get_florID_resp(\n",
        "        latcoord=ilat,\n",
        "        loncoord=ilon,\n",
        "        ntaxon=5,\n",
        "        reqtaxons=list(df_liste_especes.taxon_id)\n",
        "    )\n",
        "\n",
        "    if api_resp:\n",
        "        row_data = {\n",
        "            \"center_x\": ilon,\n",
        "            \"center_y\": ilat,\n",
        "            \"polygon_coords\": ipoly\n",
        "        }\n",
        "        for idx, taxa_id in enumerate(api_resp['req_taxa']['id']):\n",
        "            taxa_nom = api_resp['req_taxa']['name'][idx]\n",
        "            ecological_model_value = api_resp['req_taxa']['ecological_model'][idx]\n",
        "            row_data[f\"{taxa_id} {taxa_nom}\"] = ecological_model_value\n",
        "\n",
        "        # Envoyer le résultat dans la file d’attente\n",
        "        output_queue.put(row_data)\n",
        "\n",
        "# Fonction pour écrire progressivement dans un fichier CSV\n",
        "def writer(output_queue, output_file):\n",
        "    header_written = False\n",
        "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = None\n",
        "        while True:\n",
        "            row_data = output_queue.get()\n",
        "            if row_data is None:  # Indique la fin\n",
        "                break\n",
        "            if not header_written:\n",
        "                # Écrire l'en-tête\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=row_data.keys())\n",
        "                writer.writeheader()\n",
        "                header_written = True\n",
        "            # Écrire une ligne\n",
        "            writer.writerow(row_data)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-19T09:24:19.060675Z",
          "iopub.execute_input": "2024-11-19T09:24:19.061179Z",
          "iopub.status.idle": "2024-11-19T09:24:19.072604Z",
          "shell.execute_reply.started": "2024-11-19T09:24:19.061143Z",
          "shell.execute_reply": "2024-11-19T09:24:19.071387Z"
        },
        "id": "hdETBfLIzyXv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# File d’attente pour les résultats\n",
        "output_queue = Queue()\n",
        "\n",
        "\n",
        "# Lancer le thread consommateur pour écrire les résultats\n",
        "writer_thread = threading.Thread(target=writer, args=(output_queue, output_file))\n",
        "writer_thread.start()\n",
        "\n",
        "# Exécuter en parallèle les tâches de traitement\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    #executor.map(lambda ipoint: process_point(ipoint, output_queue), range(df_central_points.shape[0]))\n",
        "    executor.map(lambda ipoint: process_point(ipoint, output_queue), range(1000))\n",
        "\n",
        "# Indiquer la fin des écritures\n",
        "output_queue.put(None)\n",
        "\n",
        "# Attendre la fin du thread consommateur\n",
        "writer_thread.join()\n",
        "\n",
        "print(\"Traitement terminé et fichier CSV créé.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-19T09:24:19.074063Z",
          "iopub.execute_input": "2024-11-19T09:24:19.074428Z",
          "iopub.status.idle": "2024-11-19T09:37:39.400165Z",
          "shell.execute_reply.started": "2024-11-19T09:24:19.074397Z",
          "shell.execute_reply": "2024-11-19T09:37:39.39903Z"
        },
        "id": "2G3ssMJZzyXw",
        "outputId": "c1bdd19b-784b-424b-9123-07080e508e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Traitement terminé et fichier CSV créé.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-19T09:37:57.03701Z",
          "iopub.execute_input": "2024-11-19T09:37:57.037977Z",
          "iopub.status.idle": "2024-11-19T09:37:57.044097Z",
          "shell.execute_reply.started": "2024-11-19T09:37:57.037927Z",
          "shell.execute_reply": "2024-11-19T09:37:57.042768Z"
        },
        "id": "tDBaJC6szyXw",
        "outputId": "5987f346-49d2-4930-ab71-fd6d6d41b37e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "2024-11-19 09:37:57 \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}